from __future__ import annotations
import argparse
import json
from collections import Counter, defaultdict
from itertools import combinations
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np

# sklearn (êµ°ì§‘/ì§€í‘œ)
from sklearn.cluster import AgglomerativeClustering, SpectralClustering
from sklearn.metrics import silhouette_score


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def l2norm_nd(x: np.ndarray, axis: int = -1, eps: float = 1e-12) -> np.ndarray:
    n = np.linalg.norm(x, axis=axis, keepdims=True)
    return x / (n + eps)

def read_lines(path: Path):
    with path.open(encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if s:
                yield s

def knn_sparsify(A: np.ndarray, k: int) -> np.ndarray:
    """ê° í–‰ì—ì„œ ìƒìœ„ kê°œë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“œëŠ” í¬ì†Œí™”(ëŒ€ì¹­ ìœ ì§€)."""
    if k <= 0:
        return A
    K = A.shape[0]
    B = np.zeros_like(A, dtype=np.float32)
    for i in range(K):
        row = A[i]
        if k >= K - 1:
            idx = np.argsort(row)[::-1]
        else:
            idx = np.argpartition(row, -k)[-k:]
        B[i, idx] = row[idx]
    # ëŒ€ì¹­í™”: max(B, B^T)
    return np.maximum(B, B.T)

def balance_score(sizes: np.ndarray) -> float:
    """
    í´ëŸ¬ìŠ¤í„° í¬ê¸° ê· í˜•ì„± ì ìˆ˜(0~1): min/median ë° í€€íƒ€ì¼ ê¸°ë°˜ì˜ ê°„ë‹¨ ì§€í‘œ.
    ê°’ì´ í´ìˆ˜ë¡ ê· í˜•ì .
    """
    if len(sizes) == 0:
        return 0.0
    sizes = np.asarray(sizes, dtype=np.float64)
    if sizes.sum() == 0:
        return 0.0
    p10, p90 = np.percentile(sizes, [10, 90])
    med = np.median(sizes)
    if med <= 0:
        return 0.0
    # ë¶„ì‚°ì´ ì ê³  p10ë„ ë„ˆë¬´ ì‘ì§€ ì•Šì„ìˆ˜ë¡ ë†’ê²Œ
    score = 0.5 * (p10 / med) + 0.5 * (med / (p90 + 1e-9))
    return float(np.clip(score, 0.0, 1.0))

def eval_grouping(C_norm: np.ndarray, g: np.ndarray) -> Dict[str, float]:
    """
    êµ°ì§‘ í‰ê°€(ê°„ë‹¨): ì½”ì‚¬ì¸ ê³µê°„ì—ì„œ ì‹¤ë£¨ì—£(ìƒ˜í”Œ=centroid), ê· í˜•ì„± í˜¼í•© ì ìˆ˜.
    ë°˜í™˜: {"sil": ..., "bal": ..., "mix": ...}
    """
    # silhouette: sklearnì€ ê±°ë¦¬ ê¸°ì¤€ â†’ cosine ê±°ë¦¬ë¥¼ ì“°ë ¤ë©´ ì…ë ¥ ë²¡í„°ëŠ” L2 ì •ê·œí™” ìƒíƒœ
    try:
        sil = float(silhouette_score(C_norm, g, metric="cosine"))
    except Exception:
        sil = 0.0
    # balanceëŠ” ê° ê·¸ë£¹ í¬ê¸° ê· í˜•
    _, counts = np.unique(g, return_counts=True)
    bal = balance_score(counts)
    # í˜¼í•© ì ìˆ˜: ì‹¤ë£¨ì—£(ê°€ì¤‘ 0.7) + ê· í˜•(0.3)
    mix = 0.7 * sil + 0.3 * bal
    return {"sil": sil, "bal": bal, "mix": mix}

def summarize_super(
    super_id: int,
    members: List[int],
    A: np.ndarray,
    labels: np.ndarray,
    tokens_path: Path,
    topn_tokens: int = 100,
) -> Tuple[int, List[Tuple[str, int]]]:
    """
    ìŠˆí¼ì¹´í…Œê³ ë¦¬ ìš”ì•½: ëŒ€í‘œ(ì¤‘ì‹¬) í´ëŸ¬ìŠ¤í„°, ìƒìœ„ í† í° ëª©ë¡ ë°˜í™˜.
    ëŒ€í‘œ í´ëŸ¬ìŠ¤í„°: A ë‚´ì—ì„œ row-sumì´ ê°€ì¥ í° ë©¤ë²„.
    """
    # ëŒ€í‘œ(ì¤‘ì‹¬) í´ëŸ¬ìŠ¤í„°
    if len(members) == 1:
        central = members[0]
    else:
        row_sum = [(k, float(A[k, members].sum())) for k in members]
        row_sum.sort(key=lambda x: x[1], reverse=True)
        central = row_sum[0][0]

    # ìƒìœ„ í† í°
    token_ctr = Counter()
    # labelsì™€ tokens.txtì˜ í–‰ ì¸ë±ìŠ¤ëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ 1:1 ë§¤ì¹­ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
    with tokens_path.open(encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i >= len(labels):
                break
            c = int(labels[i])
            if c in members:
                token_ctr.update(line.strip().split())

    top = token_ctr.most_common(topn_tokens)
    return central, top

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì½”ì–´ ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_affinity(
    C: np.ndarray,
    labels: np.ndarray,
    tokens_path: Path,
    use_cooccur: bool = True,
    alpha: float = 0.7,
    knn_k: int = 10,
) -> np.ndarray:
    """
    ìµœì¢… affinity A êµ¬ì„±:
      A = alpha * S_cos + (1 - alpha) * S_co   (ë‘˜ ë‹¤ [0,1]ë¡œ ì •ê·œí™”)
    - S_cos: L2ì •ê·œí™”ëœ ì„¼íŠ¸ë¡œì´ë“œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    - S_co : ë¬¸ì¥ ë‹¨ìœ„ ê³µì¶œí˜„ ê¸°ë°˜ (ì˜µì…˜)
    - í¬ì†Œí™”: k-NN sparsify (ì˜µì…˜)
    """
    K = C.shape[0]
    Cn = l2norm_nd(C, axis=1)
    S_cos = (Cn @ Cn.T).astype(np.float32)
    np.fill_diagonal(S_cos, 0.0)
    S_cos = np.clip(S_cos, 0.0, 1.0)

    if use_cooccur:
        Co = np.zeros((K, K), dtype=np.float32)
        # ë¬¸ì¥ ë‚´ ê³ ìœ  í´ëŸ¬ìŠ¤í„° í˜ì–´ì— ëŒ€í•´ +1
        # tokens.txtëŠ” ë‹¨ìˆœíˆ ë¼ì¸ ìˆ˜ ë™ê¸°í™” ìš©ë„(ë¹ ë¥´ê²Œ ìˆœíšŒ)
        with tokens_path.open(encoding="utf-8") as f:
            for i, _ in enumerate(f):
                if i >= len(labels):
                    break
                # í•œ ë¬¸ì¥ì—ì„œ ë“±ì¥í•œ ê³ ìœ  í´ëŸ¬ìŠ¤í„° ì§‘í•©
                # (ì´ë¯¸ labelsê°€ ë¬¸ì¥â†’í´ëŸ¬ìŠ¤í„° id 1ê°œì¸ ê²½ìš°ì—” setì˜ í¬ê¸°ê°€ 1ì¼ ìˆ˜ ìˆìŒ)
                # ë§Œì•½ í† í° ë‹¨ìœ„ ë¼ë²¨ì´ ì•„ë‹ˆë¼ ë¬¸ì¥ ë‹¨ìœ„ ë¼ë²¨ì´ë¼ë©´ ê³µì¶œí˜„ íš¨ê³¼ëŠ” ì•½í•¨.
                ks = {int(labels[i])}
                if len(ks) >= 2:
                    for a, b in combinations(sorted(ks), 2):
                        Co[a, b] += 1.0
                        Co[b, a] += 1.0
        if Co.max() > 0:
            S_co = (Co / Co.max()).astype(np.float32)
        else:
            S_co = Co
    else:
        S_co = np.zeros_like(S_cos, dtype=np.float32)

    A = alpha * S_cos + (1.0 - alpha) * S_co
    A = np.clip(A, 0.0, 1.0)

    if knn_k > 0 and knn_k < K:
        A = knn_sparsify(A, k=knn_k)
    return A

def cluster_super(A: np.ndarray, method: str, M: int, random_state: int = 42) -> np.ndarray:
    K = A.shape[0]
    if method == "agglo":
        # cosine affinity â†’ distance = 1 - A
        D = 1.0 - A
        np.fill_diagonal(D, 0.0)

        # âœ… sklearn ìµœì‹ : metric='precomputed' ì‚¬ìš©
        try:
            mdl = AgglomerativeClustering(
                n_clusters=M,
                metric="precomputed",   # new API
                linkage="average",
                compute_full_tree=False,
            )
            g = mdl.fit_predict(D)
        except TypeError:
            # ğŸ” êµ¬ë²„ì „ í˜¸í™˜: affinity='precomputed'
            mdl = AgglomerativeClustering(
                n_clusters=M,
                affinity="precomputed",  # old API
                linkage="average",
                compute_full_tree=False,
            )
            g = mdl.fit_predict(D)

        return g.astype(np.int32)

    elif method == "spectral":
        mdl = SpectralClustering(
            n_clusters=M,
            affinity="precomputed",
            assign_labels="kmeans",
            random_state=random_state,
        )
        g = mdl.fit_predict(A)
        return g.astype(np.int32)

    else:
        raise ValueError(f"unknown method: {method}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    ap = argparse.ArgumentParser(description="í´ëŸ¬ìŠ¤í„° â†’ ìŠˆí¼ì¹´í…Œê³ ë¦¬ ìë™ ì••ì¶•")
    ap.add_argument("--centroids", type=Path, required=True, help="centroids.npy (K,H)")
    ap.add_argument("--labels", type=Path, required=True, help="labels.npy (N,)")
    ap.add_argument("--tokens", type=Path, required=True, help="tokens.txt (N lines)")
    ap.add_argument("--out_map", type=Path, required=True, help="ì¶œë ¥: super_map.json")
    ap.add_argument("--preview", type=Path, required=True, help="ì¶œë ¥: super_preview.txt")
    ap.add_argument("--out_labels", type=Path, default=None, help="(ì˜µì…˜) super_labels.npy")
    ap.add_argument("--alpha", type=float, default=0.7, help="A = Î±Â·S_cos + (1-Î±)Â·S_co")
    ap.add_argument("--knn_k", type=int, default=10, help="Affinity kNN sparsify (0=off)")
    ap.add_argument("--method", choices=["agglo", "spectral"], default="agglo")
    ap.add_argument("--cands", type=str, default="12,14,16",
                    help="ìŠˆí¼ì¹´í…Œê³ ë¦¬ í›„ë³´ ê°œìˆ˜(ì½¤ë§ˆêµ¬ë¶„)")
    ap.add_argument("--random_state", type=int, default=42)
    ap.add_argument("--topn_tokens", type=int, default=120)
    args = ap.parse_args()

    # 0) ë¡œë“œ
    C = np.load(args.centroids)  # (K,H)
    if C.ndim != 2:
        raise ValueError("centroids.npy must be 2D (K,H)")
    K, H = C.shape
    print(f"[load] centroids: {args.centroids}  shape={C.shape}")

    y = np.load(args.labels)     # (N,)
    if y.ndim != 1:
        raise ValueError("labels.npy must be 1D (N,)")
    N = len(y)
    print(f"[load] labels: {args.labels}  shape={y.shape}  (N={N})")

    # 1) Affinity
    A = build_affinity(
        C=C,
        labels=y,
        tokens_path=args.tokens,
        use_cooccur=True,
        alpha=args.alpha,
        knn_k=args.knn_k,
    )
    print(f"[affinity] alpha={args.alpha}  knn_k={args.knn_k}  method={args.method}")

    # 2) ìŠ¤ìœ•í•˜ì—¬ ìµœì  M ì„ íƒ
    Cn = l2norm_nd(C, axis=1)
    cand_M = [int(s) for s in args.cands.split(",") if s.strip()]
    best = None
    log_rows = []
    for M in cand_M:
        g = cluster_super(A, args.method, M, args.random_state)
        scores = eval_grouping(Cn, g)
        row = {"M": M, **scores}
        log_rows.append(row)
        mix = scores["mix"]
        if (best is None) or (mix > best[0]):
            best = (mix, M, g, scores)
        print(f"[sweep] M={M:2d}  sil={scores['sil']:.4f}  bal={scores['bal']:.4f}  mix={scores['mix']:.4f}")

    # 3) ìµœì¢… ì„ íƒ
    assert best is not None
    _, M_best, g_best, best_scores = best
    print(f"[choose] M={M_best}  sil={best_scores['sil']:.4f}  bal={best_scores['bal']:.4f}  mix={best_scores['mix']:.4f}")

    # 4) ì €ì¥: super_map.json
    args.out_map.parent.mkdir(parents=True, exist_ok=True)
    super_map = {str(k): int(g_best[k]) for k in range(K)}
    args.out_map.write_text(json.dumps(super_map, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"[save] super_map -> {args.out_map}")

    # 5) (ì˜µì…˜) ë¬¸ì¥ ë‹¨ìœ„ super_labels.npy
    if args.out_labels:
        sup_labels = np.array([g_best[int(c)] for c in y], dtype=np.int32)
        Path(args.out_labels).parent.mkdir(parents=True, exist_ok=True)
        np.save(args.out_labels, sup_labels)
        print(f"[save] super_labels -> {args.out_labels}  shape={sup_labels.shape}")

    # 6) í”„ë¦¬ë·° íŒŒì¼
    args.preview.parent.mkdir(parents=True, exist_ok=True)
    with args.preview.open("w", encoding="utf-8") as fw:
        fw.write("=== SUPER CATEGORIES PREVIEW ===\n")
        fw.write(f"centroids: {args.centroids} (K={K}, H={H})\n")
        fw.write(f"labels   : {args.labels} (N={N})\n")
        fw.write(f"tokens   : {args.tokens}\n")
        fw.write(f"alpha={args.alpha}  knn_k={args.knn_k}  method={args.method}\n")
        fw.write("sweep logs:\n")
        for r in log_rows:
            fw.write(f" - M={r['M']:2d}  sil={r['sil']:.4f}  bal={r['bal']:.4f}  mix={r['mix']:.4f}\n")
        fw.write("\n")

        # ê·¸ë£¹ë³„ ìš”ì•½
        for s in range(M_best):
            members = [k for k in range(K) if g_best[k] == s]
            central, top = summarize_super(
                super_id=s,
                members=members,
                A=A,
                labels=y,
                tokens_path=args.tokens,
                topn_tokens=args.topn_tokens,
            )
            fw.write(f"[super {s}]\n")
            fw.write(f" members: {sorted(members)}\n")
            fw.write(f" central: {central}\n")
            # ìƒìœ„ í† í° 40ê°œë§Œ í”„ë¦°íŠ¸
            top40 = ", ".join([f"{w}:{c}" for w, c in top[:40]])
            fw.write(f" top_tokens: {top40}\n\n")

    print(f"[save] preview -> {args.preview}")


if __name__ == "__main__":
    main()
