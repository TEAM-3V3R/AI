import os
import json
import datetime
from pathlib import Path
from typing import Any, Dict, Tuple

import numpy as np
import torch

# --- í”„ë¡œì íŠ¸ ë‚´ import (AI.ì ‘ë‘ì–´/ë¬´ì ‘ë‘ì–´ ë‘˜ ë‹¤ ì§€ì›) ---
try:
    from AI.prompt_analyzer.fluency import compute_fluency
    from AI.prompt_analyzer.persistence import compute_persistence
except Exception:
    from prompt_analyzer.fluency import compute_fluency  # type: ignore
    from prompt_analyzer.persistence import compute_persistence  # type: ignore

from transformers import AutoTokenizer, AutoModel

# KoBERT: fast ë¯¸ì§€ì› â†’ ì¼ê´€ ì„¤ì •
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("TRANSFORMERS_NO_TF", "1")
os.environ.setdefault("TRANSFORMERS_NO_FLAX", "1")
os.environ.setdefault("TRANSFORMERS_NO_JAX", "1")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì„¼íŠ¸ë¡œì´ë“œ ê²½ë¡œ ì¶”ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_THIS_DIR = Path(__file__).resolve().parent
_DEFAULT_CENTROIDS_CANDIDATES = [
    _THIS_DIR / "DPDT" / "models" / "kmeans_k100" / "centroids.npy",
    _THIS_DIR.parent / "DPDT" / "models" / "kmeans_k100" / "centroids.npy",
    Path("AI/DPDT/models/kmeans_k100/centroids.npy"),
    Path("DPDT/models/kmeans_k100/centroids.npy"),
]

def _default_centroids_path() -> str:
    for p in _DEFAULT_CENTROIDS_CANDIDATES:
        if Path(p).exists():
            return str(p)
    # ìµœí›„ì˜ í´ë°±(ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
    return "DPDT/data/centroids.json"

DEFAULT_CENTROIDS_PATH = _default_centroids_path()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ì—­ ë¦¬ì†ŒìŠ¤ (lazy)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_TOKENIZER = None
_MODEL = None
_DEVICE = None
_CENTROIDS = None          # np.ndarray(float32) (K, H)
_CENTROIDS_TAG = None      # ê²½ë¡œ í•´ì‹œ

def _ensure_resources(model_name: str, centroids_path: str) -> None:
    """ì „ì—­ ë¦¬ì†ŒìŠ¤ 1íšŒ ë¡œë“œ & ì¬ì‚¬ìš©"""
    global _TOKENIZER, _MODEL, _DEVICE, _CENTROIDS, _CENTROIDS_TAG

    if _TOKENIZER is None or _MODEL is None:
        # KoBERTëŠ” SentencePiece â†’ use_fast=False
        _TOKENIZER = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        _MODEL = AutoModel.from_pretrained(model_name).eval()
        _DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        _MODEL.to(_DEVICE)

    cp = Path(centroids_path)
    tag = (cp.resolve().as_posix() if cp.exists() else str(cp))
    if (_CENTROIDS is None) or (_CENTROIDS_TAG != tag):
        if not cp.exists():
            raise FileNotFoundError(f"centroids not found: {cp}")
        arr = np.load(cp)
        if arr.dtype != np.float32:
            arr = arr.astype(np.float32, copy=False)
        _CENTROIDS = arr
        _CENTROIDS_TAG = tag

def _call_metric(func, texts, centroids_path: str, model_name: str) -> Tuple[float, float, float, float]:
    """
    compute_fluency / compute_persistence í˜¸ì¶œ í˜¸í™˜ ë˜í¼
    - ì‹ ë²„ì „: (score, S, K, C) ë°˜í™˜
    - êµ¬ë²„ì „: score(float)ë§Œ ë°˜í™˜
    - resources ì¸ìë¥¼ ì§€ì›í•˜ë©´ ì „ë‹¬(ì†ë„â†‘), ì•„ë‹ˆë©´ ì¼ë°˜ í˜¸ì¶œ
    """
    # ìš°ì„  resourcesì™€ í•¨ê»˜ í˜¸ì¶œ ì‹œë„
    try:
        val = func(
            texts,
            centroids_path,
            model_name,
            resources={
                "tokenizer": _TOKENIZER,
                "model": _MODEL,
                "device": _DEVICE,
                "centroids": _CENTROIDS,
            },
        )
    except TypeError:
        # êµ¬ë²„ì „ ì‹œê·¸ë‹ˆì²˜ë¡œ ì¬ì‹œë„
        val = func(texts, centroids_path, model_name)

    # ë°˜í™˜ê°’ ì •ê·œí™”
    if isinstance(val, tuple) and len(val) >= 4:
        score, S, K, C = val[:4]
        return float(score), float(S), float(K), float(C)
    else:
        return float(val), 0.0, 0.0, 0.0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì™¸ë¶€ ê³µê°œ ë‹¨ì¼ API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_from_api(
    texts,
    centroids_path: str = DEFAULT_CENTROIDS_PATH,
    model_name: str = "skt/kobert-base-v1",
) -> Dict[str, Any]:
    print("ğŸ“¥ analyze_from_api ì§„ì…", flush=True)

    if not texts:
        print("âš ï¸ ì…ë ¥ ë¬¸ì¥ ì—†ìŒ", flush=True)
        return {
            "error": "ë¶„ì„í•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.",
            "status": 400,
            "timestamp": datetime.datetime.now().isoformat(),
        }

    try:
        _ensure_resources(model_name, centroids_path)

        print("ğŸ§  compute_fluency ì‹œì‘", flush=True)
        flu_score, fS, fK, fC = _call_metric(compute_fluency, texts, centroids_path, model_name)
        print("âœ… compute_fluency ì™„ë£Œ:", flu_score, flush=True)

        print("ğŸ§  compute_persistence ì‹œì‘", flush=True)
        pers_score, pS, pK, pC = _call_metric(compute_persistence, texts, centroids_path, model_name)
        print("âœ… compute_persistence ì™„ë£Œ:", pers_score, flush=True)

        creativity_score = (flu_score * 0.5 + pers_score * 0.5)
        print("ğŸ¯ ìµœì¢… ì ìˆ˜:", creativity_score, flush=True)

        return {
            "fluency": round(flu_score, 4),
            "persistence": round(pers_score, 4),
            "creativity": round(creativity_score, 4),
            "message": "ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "status": 200,
            "timestamp": datetime.datetime.now().isoformat(),
            "detail": {
                "fluency_SKC": {"fluency_S": round(fS, 4), "fluency_K": round(fK, 4), "fluency_C": round(fC, 4)},
                "persistence_SKC": {"persistence_S": round(pS, 4), "persistence_R": round(pK, 4), "persistence_F": round(pC, 4)},
            },
        }

    except Exception as e:
        print("âŒ ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ:", e, flush=True)
        return {
            "error": str(e),
            "status": 500,
            "timestamp": datetime.datetime.now().isoformat(),
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œì»¬ ì…€í”„ í…ŒìŠ¤íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    samples = [
        "ì•ˆê°œ ë‚€ ìˆ²ê¸¸ì„ í™€ë¡œ ê±·ëŠ” ì‚¬ëŒ",
        "ê°•ì•„ì§€ê°€ ë›°ë…¸ëŠ” í‘¸ë¥¸ ë“¤íŒ",
        "ë„ì‹œì˜ ë°¤ê±°ë¦¬ë¥¼ ë‹¬ë¦¬ëŠ” ìë™ì°¨",
        "ì•„ì´ë“¤ì´ ê³µì›ì—ì„œ ë›°ì–´ë…¸ëŠ” ì¥ë©´",
        "ë°”ë‹·ê°€ì—ì„œ ì„œí•‘ì„ ì¦ê¸°ëŠ” ì²­ë…„",
        "ì±…ìƒ ìœ„ì— í¼ì³ì§„ ê³ ì„œì™€ ë§Œë…„í•„",
        "í•˜ëŠ˜ ë†’ì´ ë–  ìˆëŠ” ì—´ê¸°êµ¬",
        "ë°¤í•˜ëŠ˜ì„ ìˆ˜ë†“ëŠ” ë¶ˆê½ƒë†€ì´",
        "ìœ ë¦¬ì°½ ë„ˆë¨¸ë¡œ ë¹„ê°€ ë‚´ë¦¬ëŠ” í’ê²½",
        "ì‚° ì •ìƒì—ì„œ ì¼ì¶œì„ ë°”ë¼ë³´ëŠ” ë“±ì‚°ê°",
    ]
    out = analyze_from_api(samples, centroids_path=DEFAULT_CENTROIDS_PATH, model_name="skt/kobert-base-v1")
    print(json.dumps(out, ensure_ascii=False, indent=2))
